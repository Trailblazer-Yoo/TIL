# TIL(Today I Learned)

___

> Mar/3rd/2022_Multi campus_유선종 Day51

## 손실함수
우리는 손실함수를 이미 알고 있다. 머신러닝을 배웠다면 지도학습 중에서 회귀에 해당하는 MSE가 손실함수라는 것을 알고 있을 것이다. 딥러닝에서 손실함수는 MSE도 사용되지만 주로 교차 엔트로피가 사용된다. 그러므로 우리는 MSE보다는 교차 엔트로피를 사용하게 된다. 오늘은 엔트로피에 대한 이해를 하고 넘어가겠다.
> MSE가 뭔지 모른다면 Day43을 참고하자.

### 1. 엔트로피(entropy)
- 우리는 MSE가 추정된 회귀선과 실제 데이터의 차이라는 것을 알고 있다. 즉, MSE는 통계에서 나온 개념이다.
- 반면에, 엔트로피는 열역학, 최근에는 IT분야의 정보 이론에서 나온 말이다. 이제부터 머리가 아픈 이야기를 할 것이므로 마음 단단히 먹자.

#### 1. 정보 엔트로피
- 엔트로피는 Shannon(새넌)이 고안한 개념으로 __정보의 양__ 을 의미한다.
- 특히, 정보 엔트로피는 각 메세지에 포함된 정보의 기댓값을 의미한다. 여기서 기댓값은 평균을 의미한다. 쉽게 예를 들어보자.

1. 우리는 엔트로피를 정보의 양이라고 말했다. 그렇다면 우리가 어떤 사건을 마주할 때 정보의 양이 많은 상황일까?
2. 답은 우리에게 흔하게 일어나지 않을 경우에 정보의 양이 많다.
    - 예를 들어, 우리가 아침, 점심, 저녁을 매일 먹는 것은 너무나도 당연하다.
    - 반면에, 오히려 아침, 점심, 저녁을 먹지 않는 경우는 흔하지 않다. 일이 바빠서 혹은 다이어트를 하는 등의 특수한 상황이 아니면 밥을 거르지 않는다. ~~밥에 진심인 한국인이다.~~
    - 친구한테 "아침 먹었어." 라고 물었을때 "응 먹었어." 하면 대화는 거기서 끝이다.
    - 반면에, "아니 안 먹었어." 라고 대답이 왔을 때, 우리는 "왜?"라고 되물으면서 추가적인 정보를 얻고자 한다.
    - 즉, __정보는 평소에 자주 일어나지 않을 때 많다.__
3. 평소에 자주 일어나지 않을 때 정보가 많다는 것을 수학적으로 바꿔보자면 사건이 일어날 확률이 적을 때 정보의 양이 많다고 바꿀 수 있다. 반대로 확률이 높다면 정보의 양은 적다. 밑의 이미지는 이러한 역의 상관관계를 그래프로 나타낸 것이다.

<img src="https://user-images.githubusercontent.com/97590480/156572606-08946804-bf96-437c-9457-227d2f2b47c0.png">

___

- 우리의 목적은 __엔트로피__ 에 대해서 알아보는 것이다. 엔트로피의 정의는 정보의 __기댓값__ 이라고 말했다. 좀더 쉽게 얘기하면 정보의 평균값이다.
- 기댓값이 들어갔다고 어려워하지 말자. 본질은 정보의 양이다.
- 어떤 사건이 발생할 확률이 상대적으로 높다면 정보의 양도 상대적으로 적기 때문에 정보의 기댓값 또한 적다. 정보의 양과 엔트로피의 관계는 양의 상관관계이다.
- 엔트로피를 달리 정의하면 __불확실성__ 을 의미한다. 엔트로피가 높다는 것은 정보의 기댓값(양)이 높다는 것을 의미하고, 이는 발생할 확률이 낮다는 것을 의미한다. 발생활 확률이 낮다는 것은 평소에 자주 보지 못하는 특수한 상황이라는 것을 의미하므로 불확실성이 커졌다고 표현할 수 있다.

___

#### 2. 엔트로피 공식
엔트로피에서는 로그를 사용한다는 것이 특이하고 나머지는 우리가 통계에서 연속확률분포의 기댓값을 구하는 공식과 동일하다. 식으로 나타내면 다음과 같다.

<img src="https://user-images.githubusercontent.com/97590480/156575649-95dc6449-92c0-4961-b0f9-f005af61b333.png">

1. 엔트로피는 정보량과 양의 상관관계를 갖고 확률과 음의 상관관계를 갖는다. 그러므로 정보량은 확률과 반비례 관계이므로 확률이 분모에 위치하게 된다.
2. 여기서는 자연로그를 취하는데, 밑이 e인 경우에 정보량의 단위를 내트(nat)라 한다. 만약, 2진수 데이터를 나타내기 위해 밑이 2인 경우에는 정보량의 단위는 비트(bit)가 된다.

___

#### 3. 정보량에 로그를 사용하는 이유
1. 정보량과 확률은 반비례 관계이기 때문이다. 보통 우리가 다루는 확률에서 확률이 높을수록 사건의 발생 횟수가 증가하는 등 확률은 정비례 관계인 경우가 대부분이다. 그러나 여기서는 반비례 관계이기 때문에 기울기가 마이너스인 관계식이 필요하다.
2. 로그는 독립사건을 표현할 때 아주 유용하다. ln(p1p2) = ln(p1) + ln(p2)이므로 독립사건의 기본가정을 잘 표현할 수 있는 관계식이다.
3. p = 1일때, 정보량은 0이다. 이러한 관계를 로그를 이용해 표현할 수 있다. ln(1) = 0 이므로 엔트로피에서는 로그를 취함으로써 그 성질을 표현한다.

___

### 3. 교차 엔트로피(cross entropy)
교차 엔트로피의 식은 다음과 같다.

<img src="https://user-images.githubusercontent.com/97590480/156578438-7442de06-7eed-453d-85a5-8b303ce139c8.png">

1. 여기서 tk는 정답 레이블, yk는 신경망 출력값을 의미한다.
2. 신경망 출력값은 확률값이다(소프트맥스 함수를 생각해보자). 즉, 우리가 위에서 알아봤던 확률이 여기서는 신경망 출력값이 된다.
3. 만약 정답 레이블이 0이라면 전체 값은 0이 될 것이다.
4. 반대로 정답 레이블이 1이라면 전체 값은 lnyk로 신경망 출력값에 대한 엔트로피 값이 나온다.
5. 회귀식과 떨어진 정도를 나타내는 MSE와 다르게 교차 엔트로피는 정답에 대한 정보를 이용하여 정확도를 측정한다.
   - 예를 들어보자. 만약 정답 레이블이 [0,0,0,1]이고, 신경망 출력값이 [0.1, 0.2, 0.5, 0.3]으로 나왔다고 치자.
   - 그렇다면 교차 엔트로피값은 0 x -ln(0.1) + 0 x -ln(0.2) + 0 x -ln(0.5) + 1 x -ln(0.3) = -ln(0.3) = 0.52 가 나오게 된다.
   - 즉, 정답의 새로운 정보량이 52%라는 것을 의미하고, 교차 엔트로피 값이 1에 가까울수록 새로운 정보를 얻는 것이 많아지는 것을 의미한다. 즉, 교차 엔트로피 값이 0이 나와야 우리가 당연하게 생각하는 결과, 정답 레이블과 신경망 출력값이 일치하는 결과가 나온다는 것을 의미한다.
6. 위의 식에 데이터의 갯수로 나눠준 것이 범주형 교차 엔트로피 오차(Categorical Cross Entropy Error)이며, 소프트맥스 함수와 같이 손실함수에 사용된다.

___

코딩을 해보면 다음과 같다.

```python
def CEE(predict, label):
    delta = 1e-4 # 0.0001
    return -np.sum(label * np.log(predict + delta))
```
여기서 델타는 값이 너무 작아져서 -inf 값이 나오는 것을 방지하기 위한 모수이다.

___

### 4. 이진 교차 엔트로피 오차(binary cross entropy Error)
1. 위에서 본 교차 엔트로피는 신경망 출력값이 확률값이므로 소프트맥스 함수를 활성화 함수로 갖는 신경망 출력층에 적절하다.
2. 반면에, 시그모이드 함수처럼 값이 0 또는 1로 두개로 나오는 경우가 있는데, 이때 사용하는 손실함수가 이진 교차 엔트로피 오차이다.
3. 식은 다음과 같다.

<img src="https://user-images.githubusercontent.com/97590480/156582518-76e0dec7-b3bd-47dd-b69e-0949cf93fb74.png">

> 식이 복잡해보이지만 y값이 1,0인 경우를 다루므로 값이 (y, 1-y)일 경우를 더해줬다고 생각하면 좀더 쉽다.