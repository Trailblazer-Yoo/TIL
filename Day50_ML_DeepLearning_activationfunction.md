# TIL(Today I Learned)

___

> Mar/2nd/2022_Multi campus_유선종 Day50

## 활성화 함수(activation function)
우리는 어제 퍼셉트론을 통해 딥러닝의 구조를 파악했다. 어떠한 입력값을 가중치를 통해 값을 계산한 후 출력해주는 구조는 퍼셉트론과 딥러닝 동일하다. 그러나 딥러닝은 퍼셉트론의 가중치를 결정하는 구조가 추가된 신경망 구조를 기반으로 진행된다. 오늘은 신경망 구조에서 입력과 출력 사이에 값을 결정하는 활성화 함수에 대해서 알아보고자 한다.

### 1. 활성화 함수의 정의
활성화 함수는 입력 신호의 총합을 출력 신호로 변환하는 함수를 말한다. 즉, 식으로 나타내면 다음과 같다.   
`a = b + w1x1 + w2x2`   
`y = h(a)`   
1. 여기서 b는 편향(bias)로 b * 1로 나눠서 b는 가중치, 1는 입력값이라고 생각하면 w1x1과 동일한 형식으로 표현할 수 있다.
2. `y = h(a)`에서 a는 입력, y는 출력, h()는 함수인데, h()는 여기서 활성화함수를 의미한다. 이미지로 나타내면 다음과 같다.

<img src="https://user-images.githubusercontent.com/97590480/156293482-dd358888-e656-4c17-b5b3-79ca87f37895.png">

___
### 2. 계단 함수(step function)
딥러닝에서 활성화 함수를 이용해서 출력값을 구하듯, 퍼셉트론에서도 활성화 함수를 이용해서 출력값을 구한다. 이때, 사용하는 활성화 함수는 계단 함수(step function)이다. 식으로 나타내면 다음과 같다.

<img src="https://user-images.githubusercontent.com/97590480/156293766-b781503f-4e1c-4e1d-87d1-012837f4ebdc.png">

- 이를 파이썬을 이용한 함수로 표현하면 다음과 같다.

```python
def step_function(x):
    if x > 0: return 1
    elif x <= 0: return 0
```

- 그림으로 나타내면 다음과 같다.

<img src="https://user-images.githubusercontent.com/97590480/156293916-d497d0ac-218b-468f-b613-d90e4c768c91.png">

- x값이 0인 지점에서 y값이 0과 1로 나뉘는 것을 볼 수 있다.

___

### 3. 시그모이드 함수(sigmoid function)
시그모이드 함수는 로지스틱 회귀에서 로그오즈를 이용해서 도출한 함수이다. 로지스틱 회귀에 대해서 서술한 적이 없으므로 로지스틱 회귀에 대해서 설명하고 넘어가겠다.

#### 1. 로지스틱 회귀
로지스틱 회귀는 머신러닝에서 지도학습에 속하고 그 중에서도 분류에 속하는 기법이다. 이름에서도 알 수 있듯이 회귀의 성질을 가지면서도 결과값을 0과 1로 출력해서 분류 기법에 사용된다. 이 특징이 매우매우매우 중요하다. 하나하나 살펴보자.

<img src="https://user-images.githubusercontent.com/97590480/156295215-3b1432a5-cd47-4e5a-a4b0-759276da7b62.png">

> ln은 밑을 자연상수 e로 갖는 로그이다. 또한, exp(x)는 exponential의 약자로 e^x를 의미한다.

1. 위의 이미지는 로그오즈에서 시그모이드를 도출하는 과정이다. 여기서 사건은 성공/실패 등 전체 시행 중에서 두가지 사건만 일어날 때를 다룬다.
2. 로그오즈는 사건이 발생(1)할 확률 p에서 사건이 발생하지 않을(0) 확률 1-p로 나눈 것을 자연로그 취한 것을 말한다.
    - 단순히 p로만 하면 되는데 왜 로그오즈로 변환해서 사용하는지에 대한 궁금증이 생길 수 있다. 그 이유는 위에서 회귀식이 (-∞, ∞)범위를 가지기 때문이다.
    - p는 `0 <= p <= 1`의 범위를 갖지만 오른쪽의 회귀식은 (-∞, ∞)의 범위를 갖는다. 즉 일대일 대응이 불가능하다.
    - 이를 일대일 대응으로 맞춰주기 위해서 p를 (-∞, ∞)범위로 바꿔줘야 한다.
    - p/(1-p)를 해줄경우 p가 0이라면 p/(1-p)의 값은 0이 되고, p가 1이라면 p/(1-p)의 값은 ∞가 된다. 여기서 p/(1-p)의 범위는 (0, ∞)이다.
    - 여기에 로그를 취해줄경우 (0,1)범위는 분수이므로 분수를 로그 취해주면 마이너스값이 되기 때문에 ln(p/(1-p))의 범위는 (-∞, ∞)로 회귀식과 일대일 대응이 된다.
3. 로그오즈를 p로 정리해주면 위의 식처럼 나온다. 이를 시그모이드라고 부른다.
4. 이것을 딥러닝에 적용하면 p는 출력값, x는 입력값, h()는 활성화 함수인 시그모이드 함수이다.
   - 여기서 중요한 것은 p를 출력값으로 갖는다는 것이다. 만약 시그모이드 함수와 계단 함수를 활성화함수로 갖는 딥러닝이라면 p의 의미가 더욱 잘 보인다.
   - 시그모이드는 확률값을 출력값으로 출력하고 계단 함수를 거치면서 어떤 임계값보다 높다면 1, 낮다면 0을 결과값으로 갖는다. 이것은 현실을 매우 잘 설명해주는 함수이다.
   - 예를들어, 우리가 삼성전자 주식을 산다고 생각해보자. 산다는 행위는 1, 안산다는 행위는 0의 결과값을 갖는다.
   - 이때, 우리는 삼성전자의 가격이 지금보다 올라갈 확률이 내려갈 확률보다 크다면 사는 행위가 매우 적절하고 당연한 결과일 것이다. ~~확률을 결정하는 것은 재무제표, 전망 등일 것이다.~~
   - 그러므로 우리는 확률 p의 출력값이 어떤 임계값보다 더 높다면 그 행위를 실행하는 1으로 결과값을 가지게 된다.
5. 로지스틱 회귀는 이 로그오즈를 이용해서 임계값을 0.5로 설정하여 0.5 이상이면 1, 0.5 미만이면 0을 출력하는 분류 알고리즘이다. 이를 시각화하면 다음과 같다.

<img src="https://user-images.githubusercontent.com/97590480/156297330-41f352e4-d85a-47e1-8c5c-b4d4c60cc3e3.png">

___

#### 2. 시그모이드 함수
시그모이드 함수를 시각화하면 위의 그래프와 동일하며, 로지스틱 회귀에서 사용되는 시그모이드 함수를 활성화 함수로 적용하여 파이썬에 구현해보면 다음과 같다.

```python
import numpy as np
def sigmoid(x):
    return 1/ (1 + np.exp(-x))
```
1. x에 값을 넣어주면 시그모이드 함수 출력값으로 바꿔주고, 그 범위는 `0 <= y <= 1`이다.
2. 계단함수와는 다르게 0과 1 사이에 부드러운 곡선 부분이 생겼다. 즉, 비선형 함수이다.
3. 퍼셉트론에서 다층 퍼셉트론에서는 비선형 구조를 가진다고 했었다. 앞으로 딥러닝에서 사용하는 모든 활성화함수는 비선형 함수를 사용한다.

___

### 4. 소프트맥스 함수(softmax function)
- 소프트맥스 함수(softmax function)는 출력값이 3개 이상일 때 사용한다. 예를 들어, 저소득층, 중산층, 고소득층의 3개로 나누고 싶다면 소프트맥스 함수를 사용한다.
- 소프트맥스 함수의 식은 다음과 같다.

<img src="https://user-images.githubusercontent.com/97590480/156302174-ea76e51d-0119-4dd2-a03f-687a94b6aa4f.png">

> 왜 여기서 지수함수를 사용하는지 궁금하다면 maxium entropy와 Boltzmann distribution을 검색해보자.

1. 여기 식에서 n은 출력층의 갯수, k는 해당 출력층을 의미한다. 예를 들어 10개의 출력층에서 3번째 출력층의 출력값을 구하고 싶다면 n = 10, k = 3이다.
2. 식을 보면 분모는 모든 출력층의 합을 의미하므로 출력층의 합은 1이고, 출력값은 비중(weight)를 의미한다.
3. 예를들어, 3번째 출력층의 값이 0.3이 나왔다면 전체 출력층에서 30%를 차지한다고 말할 수 있다. 어제 말한 가중치에서 비중의 의미는 소프트맥스 함수에서 사용된다.
4. 비중은 다른 말로 바꿔 말하면 확률로 말할 수 있다. 전체 시행 중에서 3번째 출력층이 발생할 확률은 0.3, 30%를 갖는다고 말할 수 있다.
> 동전을 10번 던져서 3번이 앞면, 7번이 뒷면이 나왔다면 앞면이 나올 확률은 0.3이고, 뒷면이 나올 확률은 0.7이다. 이는 10번을 1로 바꾸고, 3을 0.3, 7을 0.7로 바꾼 것과 다름이 없다.
5. 소프트맥스 함수는 지수함수를 사용하기 때문에 그 값이 매우 커지는 오버플로우(overflow) 문제가 발생한다. 예를 들어, e^1000 이라면 무한대에 가까워 무한대를 의미하는 inf 값으로 바꿔준다. 이를 방지하기 위해 특정 값을 곱해줘서 변형해준다. 식은 다음과 같다.

<img src="https://user-images.githubusercontent.com/97590480/156303307-32c5aaea-896e-473e-8599-f862af0d72f5.png">

6. C에 어떤 값을 넣어도 상관없지만 오버플로우를 막기 위해 사용하는 모수이므로 입력 신호 중 최댓값을 C 값으로 설정해준다.
7. 위를 파이썬으로 구현해보면 다음과 같다.

```python
import numpy as np
def softmax(x):
    c = np.max(x)
    exp_x = np.exp(x - c)
    sum_exp_x = np.sum(exp_x)
    y = exp_x / sum_exp_x
    return y
```

___

### 5. 출력층에서 사용하는 활성화함수
- 우리는 다층 퍼셉트론 구조를 가지는 신경망 모형을 딥러닝에서 사용한다는 것을 알고 있다.
- 그 중에서 입력, 출력, 그리고 그 사이에 있는 것들로 구성이 되어 있는데, 이를 정확하게 이름을 지어 구분하자면 입력층(input layer), 은닉층(hidden layer), 출력층(output layer)이라 한다.
- 그 중 은닉층은 대부분 2개 이상인 경우가 많고, 활성화함수를 거치면서 가중치를 조절해나간다.
- 마지막 은닉층에서 출력층으로 넘어올 때에도 활성화함수를 사용하는데, 이때 사용하는 활성화함수가 어느정도 정형화되어 있다.
- 왜냐하면 출력층에서는 결과를 1/0 혹은 3개 이상으로 나뉘어지기 때문에 이때 사용하는 활성화함수를 우리는 위에서 배웠다.
- 즉, 결과가 1/0으로 분류처럼 두개로 나뉘는 경우에는 시그모이드 함수, 3개 이상인 경우에는 소프트맥스 함수, 결과를 그대로 출력하고 싶다면(결과값이 1개) 항등 함수(identity function)을 사용한다.
- 항등함수는 다른 것이 아니라 그냥 그 값을 다시 출력해주는 것을 의미하므로 은닉층의 결과값을 그대로 받아 출력층이 1개인 값으로 출력해주면 된다.

___

### 6. 은닉층에서 사용하는 활성화함수
- 출력층에서는 주로 시그모이드와 소프트맥스 함수를 사용한다고 했다.
- 은닉층에서 주로 사용하는 활성화 함수는 Relu 함수이다.
- Relu는 Rectified Linear Unit의 약자로 우리말로 번역하면 정류된 선형 함수이다. 정류는 전기회로에서 전기의 흐름을 차단하는 용어로 사용된다. 즉, 여기서 Relu함수는 어떤 값이 출력되지 않도록 차단(제약)을 하는 함수이다.
- Relu 함수는 x > 0 일때에는 x값을 그대로 함수값으로 갖지만, x <= 0 일때에는 함수값으로 0을 갖는다. 식과 그래프를 보면 다음과 같다.

<img src="https://user-images.githubusercontent.com/97590480/156358432-00b8b098-2331-459f-b353-ed9979a0496f.png">

<img src="https://user-images.githubusercontent.com/97590480/156358572-9edbe227-3818-4abe-970b-fdccc1de7661.png">

그래프를 보면 함수의 성질을 쉽게 알 수 있다. 이를 통해 Relu 함수의 장단점을 알 수 있다.
1. x값을 그대로 함수값으로 출력해주기 때문에 출력속도가 매우 빠르다. 이는 매우 많은 데이터를 다루는 딥러닝에 큰 장점이다.
2. 또한, 딥러닝의 계산이 가면 갈수록 특정 양수값에 수렴하는 현상이 나타나는데, 이를 기울기 소실(Vanishing Gradient)라고 한다. Relu는 이러한 기울기 손실 문제가 발생하지 않는다.
3. 그러나 단점은 음수의 값을 0으로 설정하는 바람에 가중치의 합이 음수가 되는 순간 0으로 반환되는 Dying Relu 문제가 발생한다.
4. 이를 해결한 것이 Leaky Relu 라는 것이 있다. 쉽게 생각하면 음수값을 0이 아닌 매우 축소시켜서 약하게(leaky) 만든 Relu 라고 생각하면 된다.
___
이외에도 ELU 등 다양한 활성화함수가 있지만 제일 핵심은 위에서 서술한 활성화 함수이므로 나머지는 필요한 때에 서술하도록 하겠다. 내일은 손실함수와 순전파 알고리즘에 대해서 알아보겠다.